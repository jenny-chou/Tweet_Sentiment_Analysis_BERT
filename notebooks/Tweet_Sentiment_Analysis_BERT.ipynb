{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "stupid-james",
   "metadata": {},
   "source": [
    "# BERT for Sentiment Analysis\n",
    "\n",
    "Use BERT for sentiment analysis with 1.6 million tweets.\n",
    "\n",
    "Dataset: http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
    "\n",
    "Pretrained BERT model: BERT base\n",
    "- L=12: 12 encoder layers\n",
    "- H=768: 768 embedding dimension\n",
    "- A=12: 12 self attention heads\n",
    "\n",
    "Highlights:\n",
    "- BERT tokenizer\n",
    "- BERT pretrained embedded layer\n",
    "- Bigram, trigram, and four-gram 1D Convolution model\n",
    "\n",
    "Major data preprocessing:\n",
    "1. Tokenization:\n",
    "    - Use the vocabularies in BERT base model (about 30,000 words) for BERT tokenizer\n",
    "    - Use BERT tokenizer to tokenize all tweets\n",
    "1. 3 inputs\n",
    "    - Because we're using BERT embedding layer as first layer of our model\n",
    "    - Need to fit the model with suitable input\n",
    "    - Each input/sentence has 3 parts:\n",
    "        1. Tokenized sentence\n",
    "            - Consist of:\n",
    "            - Tokenized tweet\n",
    "            - CLS token for classification \n",
    "            - SEP token to seperate sentences (but in our case there's only one sentence, so SEP will be at the end of sentence)\n",
    "        2. Position of padding/mask\n",
    "            - Sequence of 0s and 1s indicating the position of PAD token\n",
    "            - 1 for the PAD token and 0 for others\n",
    "        3. Sentence segmentation\n",
    "            - ex:\n",
    "            - first sentence corresponds to list of 0s\n",
    "            - second sentence corresponds to list of 1s\n",
    "            - third sentence corresponds to list of 0s\n",
    "            - etc.\n",
    "2. Pretrained BERT embedding layer\n",
    "    - Use pretrained BERT embedding layer as is\n",
    "    - No fine tuning\n",
    "    - trainable=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respiratory-bacon",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "## Content\n",
    "\n",
    "1. [Import dependencies](#import)\n",
    "2. [Data preprocessing](#dataprep)\n",
    "    - [Load data](#load)\n",
    "    - [Drop unused features](#drop)\n",
    "    - [Prepare inputs](#prep)\n",
    "        - [Clean tweets](#clean)\n",
    "        - [Adjust sentiment labels](#label)\n",
    "        - [BERT tokenizer](#tokenizer)\n",
    "        - [Tokenization](#token)\n",
    "        - [3 inputs](#inputs)\n",
    "        - [Data generator](#datagen)\n",
    "3. [Modeling](#model)\n",
    "4. [Training](#train)\n",
    "5. [Evaluation](#eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-dover",
   "metadata": {},
   "source": [
    "<a id=\"import\"></a>\n",
    "## 1. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "better-sound",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "\n",
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "thick-package",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_base_model = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-defeat",
   "metadata": {},
   "source": [
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-federal",
   "metadata": {},
   "source": [
    "<a id=\"dataprep\"></a>\n",
    "## 2. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-collar",
   "metadata": {},
   "source": [
    "<a id=\"load\"></a>\n",
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "vietnamese-rapid",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_url(url, save_path, chunk_size=128):\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(save_path, 'wb') as fd:\n",
    "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "            fd.write(chunk)\n",
    "            \n",
    "def extract_file(file_path, destination):\n",
    "    with ZipFile(file_path, 'r') as file: \n",
    "        # print all the contents of the zip file \n",
    "        file.printdir() \n",
    "        # extract all the files         \n",
    "        file.extractall(destination) \n",
    "        print('All the files extracted to {}'.format(destination)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "flush-virtue",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name                                             Modified             Size\n",
      "testdata.manual.2009.06.14.csv                 2010-03-04 20:20:12        74326\n",
      "training.1600000.processed.noemoticon.csv      2010-03-04 20:20:42    238803811\n",
      "All the files extracted to data\n"
     ]
    }
   ],
   "source": [
    "download_url('http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip', 'trainingandtestdata.zip')\n",
    "extract_file('trainingandtestdata.zip', \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "musical-breakfast",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id                          date     query  \\\n",
       "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "cols = [\"sentiment\", \"id\", \"date\", \"query\", \"user\", \"text\"]\n",
    "data = pd.read_csv(os.path.join('data', \"training.1600000.processed.noemoticon.csv\"), \n",
    "                   header=None,\n",
    "                   names=cols,\n",
    "                   engine='python',\n",
    "                   encoding='latin1')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-stuart",
   "metadata": {},
   "source": [
    "<a id=\"drop\"></a>\n",
    "#### Drop unused features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "greenhouse-semester",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1          0  is upset that he can't update his Facebook by ...\n",
       "2          0  @Kenichan I dived many times for the ball. Man...\n",
       "3          0    my whole body feels itchy and like its on fire \n",
       "4          0  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop unused features\n",
    "data.drop(['id', 'date', 'query', 'user'], axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-compound",
   "metadata": {},
   "source": [
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-alignment",
   "metadata": {},
   "source": [
    "<a id=\"prep\"></a>\n",
    "### Prepare inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-circle",
   "metadata": {},
   "source": [
    "<a id=\"clean\"></a>\n",
    "#### Clean tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "thick-period",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    tweet = BeautifulSoup(tweet, 'lxml').get_text()\n",
    "    # remove @tag\n",
    "    tweet = re.sub(r\"@\\S+\", ' ', tweet)\n",
    "    # remove link\n",
    "    tweet = re.sub(r\"http\\S+\", ' ', tweet)\n",
    "    # remove special char\n",
    "    tweet = re.sub(r\"[^A-Za-z ?!,.\\'\\\"]\", ' ', tweet)\n",
    "    # remove excess whitespace\n",
    "    tweet = re.sub(r\" +\", ' ', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "surface-adams",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jenny\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\bs4\\__init__.py:332: MarkupResemblesLocatorWarning: \" i just received my G8 viola exam.. and its... well... .. disappointing.. :\\..\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "C:\\Users\\jenny\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\bs4\\__init__.py:332: MarkupResemblesLocatorWarning: \"E3 ON PLAYSTATION HOME IN ABOUT AN HOUR!!!!!!!!!! \\../  \\../\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     Awww, that's a bummer. You shoulda got David ...\n",
       "1    is upset that he can't update his Facebook by ...\n",
       "2     I dived many times for the ball. Managed to s...\n",
       "3      my whole body feels itchy and like its on fire \n",
       "4     no, it's not behaving at all. i'm mad. why am...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean = data.text.apply(clean_tweet)\n",
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-surprise",
   "metadata": {},
   "source": [
    "<a id=\"label\"></a>\n",
    "#### Adjust sentiment labels\n",
    "Original data labels negative as 0, neutral as 2, and positive as 4. Change 4 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "wound-shore",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_labels = data.sentiment.values\n",
    "data_labels[data_labels == 4] = 1\n",
    "set(data_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-houston",
   "metadata": {},
   "source": [
    "<a id=\"tokenizer\"></a>\n",
    "#### BERT tokenizer\n",
    "Create a BERT tokenizer. \n",
    "\n",
    "But first create a BERT layer to have access to meta data for BERT tokenizer:\n",
    "1. Vocab size\n",
    "2. Whether to lowercase inputs when tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "billion-protein",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT layer\n",
    "bert_layer = hub.KerasLayer(bert_base_model, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "recreational-alexander",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get meta data for BERT tokenizer from BERT layer\n",
    "vocab_file =    bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "conservative-fireplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT tokenizer\n",
    "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-klein",
   "metadata": {},
   "source": [
    "<a id=\"token\"></a>\n",
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-sweet",
   "metadata": {},
   "source": [
    "Tokenize Tweet with BERT tokenizer and wrap it in a sentence. Each sentence is composed of the [CLS] token, tokenized tweet, and the [SEP] token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adjusted-columbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentence(sentence):\n",
    "    return [\"[CLS]\"] + tokenizer.tokenize(sentence) + [\"[SEP]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "massive-thanks",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [[CLS], aw, ##w, ##w, ,, that, ', s, a, bum, #...\n",
       "1    [[CLS], is, upset, that, he, can, ', t, update...\n",
       "2    [[CLS], i, dive, ##d, many, times, for, the, b...\n",
       "3    [[CLS], my, whole, body, feels, it, ##chy, and...\n",
       "4    [[CLS], no, ,, it, ', s, not, be, ##ha, ##ving...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sentences = data_clean.apply(encode_sentence)\n",
    "data_sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-retention",
   "metadata": {},
   "source": [
    "<a id=\"inputs\"></a>\n",
    "#### 3 inputs for each sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-quest",
   "metadata": {},
   "source": [
    "To prepared inputs for BERT embedding layer (first layer of our model), create 3 types of inputs:\n",
    "1. Sentence with tokens\n",
    "    - Prepared in previous step\n",
    "2. Mask\n",
    "    - Indicates the position of the padding tokens\n",
    "    - Indicates BERT to not use those data for embedding\n",
    "    - 0 for padding token, [PAD], and 1 for other tokens\n",
    "3. Segment input\n",
    "    - Indicates seperation of sentences\n",
    "    - For example, tokens in first sentence have value 0, second sentence 1, third 0, ...\n",
    "    - But in our case we only have one sentence\n",
    "    \n",
    "And we also want to shuffle all sentences and filter out sentences that are too short.\n",
    "\n",
    "Note: we want to create padded batches (pad sentences for each batch independently) instead of pad the whole dataset at once, this way we add the minimum of padding tokens possible, which also minimize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "collectible-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids(tokens):\n",
    "    return tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "def get_masks(tokens):\n",
    "    return np.char.not_equal(tokens, \"[PAD]\").astype(int)\n",
    "\n",
    "def get_segments(tokens):\n",
    "    seg_ids = []\n",
    "    current_seg_id = 0\n",
    "    for token in tokens:\n",
    "        seg_ids.append(current_seg_id)\n",
    "        if token==\"[SEP]\":\n",
    "            current_seg_id = 1-current_seg_id\n",
    "    return seg_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premium-ceremony",
   "metadata": {},
   "source": [
    "Test the functions.\n",
    "\n",
    "Expect bert_layer to accept the formatted inputs and returns:\n",
    "1. BERT representation of whole sentence\n",
    "    - Vector size should be 768 because we use BERT base model\n",
    "2. BERT representation of individual word\n",
    "    - Expect 6 length-768 vectors representing \"[CLS]\", \"Roses\", \"are\", \"red\", \".\", \"[SEP]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "democratic-request",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1, 768), dtype=float32, numpy=\n",
       " array([[-9.2793560e-01, -4.1033486e-01, -9.6575487e-01,  9.0731782e-01,\n",
       "          8.1291342e-01, -1.7417410e-01,  9.1123444e-01,  3.4195185e-01,\n",
       "         -8.7452102e-01, -9.9998927e-01, -7.7840954e-01,  9.6938503e-01,\n",
       "          9.8616052e-01,  6.3696265e-01,  9.4863129e-01, -7.5119293e-01,\n",
       "         -4.5833918e-01, -7.0810443e-01,  4.6209806e-01, -6.5792716e-01,\n",
       "          7.6041436e-01,  9.9999493e-01, -3.9686024e-01,  3.4416601e-01,\n",
       "          6.1648846e-01,  9.9440002e-01, -7.7663356e-01,  9.3831652e-01,\n",
       "          9.5945227e-01,  7.3287946e-01, -6.9343668e-01,  2.9308012e-01,\n",
       "         -9.9378556e-01, -1.6455150e-01, -9.6701938e-01, -9.9554950e-01,\n",
       "          5.3293502e-01, -6.8806088e-01,  1.3471809e-02,  2.9819820e-02,\n",
       "         -9.1835654e-01,  4.2052594e-01,  9.9998909e-01,  2.5267580e-01,\n",
       "          6.0623521e-01, -3.5075003e-01, -1.0000000e+00,  4.9758524e-01,\n",
       "         -8.9518732e-01,  9.6256083e-01,  9.4373035e-01,  9.0328515e-01,\n",
       "          1.5469949e-01,  5.8614337e-01,  5.8086026e-01, -4.0505269e-01,\n",
       "         -2.7664516e-02,  2.9804561e-01, -2.8307566e-01, -6.4742434e-01,\n",
       "         -6.5152347e-01,  5.4384702e-01, -9.5630193e-01, -9.2275023e-01,\n",
       "          9.6146280e-01,  8.2747495e-01, -3.5011208e-01, -4.0640524e-01,\n",
       "         -8.7431602e-02, -9.9874161e-02,  8.9668822e-01,  3.0093116e-01,\n",
       "         -1.5112887e-01, -8.5271347e-01,  8.0959207e-01,  4.0098903e-01,\n",
       "         -6.6160601e-01,  1.0000000e+00, -6.1624610e-01, -9.8640722e-01,\n",
       "          8.9094257e-01,  8.1115723e-01,  5.8139473e-01, -6.3387257e-01,\n",
       "          3.7819728e-01, -1.0000000e+00,  6.7635101e-01, -2.3061223e-01,\n",
       "         -9.9255252e-01,  3.8546094e-01,  6.5765035e-01, -2.9010534e-01,\n",
       "          4.4683164e-01,  6.2852412e-01, -5.5840886e-01, -6.6529477e-01,\n",
       "         -4.7227204e-01, -9.2803913e-01, -3.5447204e-01, -6.1973560e-01,\n",
       "          1.2453454e-01, -3.4890541e-01, -4.2318389e-01, -4.2083472e-01,\n",
       "          4.5658848e-01, -6.1447066e-01, -5.1524305e-01,  5.0190908e-01,\n",
       "          4.2914703e-01,  7.5982183e-01,  4.3751660e-01, -4.3359795e-01,\n",
       "          6.3096166e-01, -9.5974314e-01,  7.7387732e-01, -3.9573762e-01,\n",
       "         -9.8735452e-01, -6.7318022e-01, -9.9299645e-01,  7.7780002e-01,\n",
       "         -5.0585574e-01, -3.1999075e-01,  9.6938872e-01, -3.5161957e-01,\n",
       "          3.7909159e-01, -2.2164916e-01, -9.5150548e-01, -1.0000000e+00,\n",
       "         -8.8042665e-01, -8.3471292e-01, -2.7732065e-01, -4.7046095e-01,\n",
       "         -9.8371196e-01, -9.5673025e-01,  6.6112089e-01,  9.5602584e-01,\n",
       "          1.6218896e-01,  9.9996156e-01, -5.1120543e-01,  9.5953059e-01,\n",
       "         -5.5860943e-01, -8.0022079e-01,  8.4854364e-01, -5.5832022e-01,\n",
       "          8.3373815e-01,  2.6314878e-01, -7.3384660e-01,  3.1618938e-01,\n",
       "         -4.8330560e-01,  6.8744969e-01, -7.9488939e-01, -3.8129804e-01,\n",
       "         -8.7170589e-01, -9.4948816e-01, -3.6245990e-01,  9.5117557e-01,\n",
       "         -7.6252031e-01, -9.6127790e-01, -1.5329328e-01, -4.0246317e-01,\n",
       "         -5.6981522e-01,  8.5247588e-01,  7.9981768e-01,  5.3358620e-01,\n",
       "         -6.9654632e-01,  4.8427221e-01,  2.2444016e-01,  7.3119509e-01,\n",
       "         -8.1820786e-01, -3.5814902e-01,  5.3202838e-01, -4.4167590e-01,\n",
       "         -9.2572075e-01, -9.8760730e-01, -5.0700730e-01,  5.3148520e-01,\n",
       "          9.9382704e-01,  7.6617515e-01,  4.1239306e-01,  8.8326997e-01,\n",
       "         -3.8566628e-01,  8.8184994e-01, -9.6734506e-01,  9.8640740e-01,\n",
       "         -3.1353521e-01,  3.5736361e-01, -6.5758991e-01,  2.7009654e-01,\n",
       "         -8.5911286e-01,  2.3200338e-01,  8.6285287e-01, -9.0366209e-01,\n",
       "         -7.9461014e-01, -2.8269875e-01, -4.7658375e-01, -5.0109714e-01,\n",
       "         -8.8042200e-01,  5.4558599e-01, -4.4197717e-01, -5.7772875e-01,\n",
       "         -1.2580985e-01,  9.0603203e-01,  9.8056227e-01,  8.4425342e-01,\n",
       "          5.2011889e-01,  7.8057939e-01, -9.2341429e-01, -5.8724362e-01,\n",
       "          2.3475438e-01,  2.9774666e-01,  3.5446361e-01,  9.9621862e-01,\n",
       "         -8.0123734e-01, -2.7999818e-01, -9.3994892e-01, -9.8375195e-01,\n",
       "          3.8299225e-02, -9.2882180e-01, -2.9413754e-01, -7.0060068e-01,\n",
       "          7.6786357e-01, -3.5192129e-01,  6.5768355e-01,  5.5410695e-01,\n",
       "         -9.9045932e-01, -7.8043401e-01,  5.5027258e-01, -5.0393283e-01,\n",
       "          5.5685228e-01, -3.5322452e-01,  7.8634971e-01,  9.6960706e-01,\n",
       "         -6.5410179e-01,  7.3423296e-01,  8.8199645e-01, -9.1477185e-01,\n",
       "         -7.8253460e-01,  8.5274112e-01, -4.3866748e-01,  8.2417208e-01,\n",
       "         -7.7735430e-01,  9.9162710e-01,  9.4804835e-01,  7.7440149e-01,\n",
       "         -9.5281178e-01, -7.5240004e-01, -8.6539102e-01, -8.1066489e-01,\n",
       "         -1.9108494e-01,  6.2253710e-02,  9.3934280e-01,  6.6015518e-01,\n",
       "          5.1039261e-01,  3.0385593e-01, -7.5878578e-01,  9.9794710e-01,\n",
       "         -8.3939052e-01, -9.7382164e-01, -6.9680744e-01, -4.7122544e-01,\n",
       "         -9.9213994e-01,  9.2733681e-01,  3.1111795e-01,  6.1714786e-01,\n",
       "         -5.9324533e-01, -7.3074394e-01, -9.7408116e-01,  9.1418397e-01,\n",
       "          2.3510687e-01,  9.9023283e-01, -4.9807525e-01, -9.5973939e-01,\n",
       "         -7.6237082e-01, -9.3091327e-01, -4.3207049e-02, -2.1311620e-01,\n",
       "         -6.0608453e-01, -2.8161114e-02, -9.6971834e-01,  6.3624406e-01,\n",
       "          6.3531643e-01,  5.3790540e-01, -8.9103585e-01,  9.9930316e-01,\n",
       "          1.0000000e+00,  9.7300369e-01,  9.0139687e-01,  8.8746661e-01,\n",
       "         -9.9995875e-01, -6.9002146e-01,  9.9999791e-01, -9.9373060e-01,\n",
       "         -1.0000000e+00, -9.3751091e-01, -8.1225300e-01,  2.7066070e-01,\n",
       "         -1.0000000e+00, -2.8707895e-01, -1.5091980e-01, -9.3114036e-01,\n",
       "          8.1855476e-01,  9.7832894e-01,  9.9496543e-01, -1.0000000e+00,\n",
       "          8.8145345e-01,  9.3084311e-01, -7.0602703e-01,  9.7676718e-01,\n",
       "         -6.0832971e-01,  9.7554350e-01,  5.9301925e-01,  5.5431879e-01,\n",
       "         -2.4430698e-01,  4.2284331e-01, -9.6806622e-01, -9.1415828e-01,\n",
       "         -7.7570230e-01, -7.7975315e-01,  9.9887347e-01,  2.6752543e-01,\n",
       "         -7.7068096e-01, -9.3097043e-01,  6.9825798e-01, -1.7943580e-01,\n",
       "          1.4864454e-01, -9.6940440e-01, -3.2719958e-01,  7.6922226e-01,\n",
       "          8.3843726e-01,  2.7436277e-01,  4.4667345e-01, -6.8823373e-01,\n",
       "          4.3852505e-01, -6.9621481e-02,  2.8388533e-01,  6.9668490e-01,\n",
       "         -9.5527273e-01, -5.4968441e-01, -3.8956067e-01,  3.7522227e-01,\n",
       "         -7.6476169e-01, -9.5412296e-01,  9.6972120e-01, -4.8606631e-01,\n",
       "          9.7220576e-01,  1.0000000e+00,  7.6481318e-01, -9.1330355e-01,\n",
       "          6.5708220e-01,  4.3185192e-01, -7.0107895e-01,  1.0000000e+00,\n",
       "          8.6733687e-01, -9.8366958e-01, -5.8447182e-01,  7.7954048e-01,\n",
       "         -6.7788970e-01, -7.7452379e-01,  9.9966103e-01, -3.4109291e-01,\n",
       "         -8.1447935e-01, -6.4806890e-01,  9.8627311e-01, -9.9408919e-01,\n",
       "          9.9764299e-01, -8.9453769e-01, -9.7999716e-01,  9.6047777e-01,\n",
       "          9.4923198e-01, -6.8382794e-01, -7.1789843e-01,  2.8670642e-01,\n",
       "         -7.6004064e-01,  4.7833201e-01, -9.5196325e-01,  8.0832094e-01,\n",
       "          5.2761406e-01, -1.6766559e-01,  9.1626805e-01, -8.8789898e-01,\n",
       "         -5.9343094e-01,  3.9030761e-01, -7.7692288e-01, -3.8481775e-01,\n",
       "          9.5903808e-01,  6.7838109e-01, -4.0870264e-01, -1.9968340e-02,\n",
       "         -4.6842834e-01, -7.4114287e-01, -9.7373438e-01,  6.2325341e-01,\n",
       "          1.0000000e+00, -4.3185478e-01,  8.9434850e-01, -5.7256907e-01,\n",
       "         -1.8950084e-02,  7.2482876e-02,  6.0542119e-01,  5.6456381e-01,\n",
       "         -5.0403464e-01, -8.3365297e-01,  9.2037845e-01, -9.7066462e-01,\n",
       "         -9.9262738e-01,  8.6311930e-01,  2.3281810e-01, -3.0533800e-01,\n",
       "          9.9999928e-01,  6.5102392e-01,  3.6955851e-01,  5.1695102e-01,\n",
       "          9.8993737e-01, -5.1057599e-02,  5.1978058e-01,  9.1351926e-01,\n",
       "          9.8934424e-01, -4.0651408e-01,  6.7222756e-01,  8.6624604e-01,\n",
       "         -9.6332067e-01, -3.9390528e-01, -7.3253411e-01,  6.6649720e-02,\n",
       "         -9.5042896e-01,  5.3676780e-02, -9.6452355e-01,  9.7859114e-01,\n",
       "          9.7252512e-01,  5.0241280e-01,  3.4261227e-01,  8.2006657e-01,\n",
       "          1.0000000e+00, -8.3706731e-01,  5.9741127e-01, -4.1720152e-01,\n",
       "          8.8128597e-01, -9.9991095e-01, -8.3777791e-01, -4.6696192e-01,\n",
       "         -2.7249640e-01, -9.0381408e-01, -4.5863765e-01,  3.9183319e-01,\n",
       "         -9.7905928e-01,  9.1019607e-01,  8.2955509e-01, -9.9289376e-01,\n",
       "         -9.9393326e-01, -5.5882066e-01,  7.8601187e-01,  2.9860064e-01,\n",
       "         -9.9431443e-01, -8.1672519e-01, -6.5843177e-01,  9.0782166e-01,\n",
       "         -4.8459575e-01, -9.5957869e-01, -5.2470011e-01, -4.2652306e-01,\n",
       "          5.3944707e-01, -3.5142931e-01,  6.0398811e-01,  8.8423634e-01,\n",
       "          6.9196045e-01, -7.7355313e-01, -3.4998590e-01, -1.8210557e-01,\n",
       "         -8.0959237e-01,  9.0684140e-01, -8.0970627e-01, -9.7624755e-01,\n",
       "         -2.7070537e-01,  1.0000000e+00, -5.5433255e-01,  8.9376020e-01,\n",
       "          7.5522965e-01,  7.8031617e-01, -1.9922525e-01,  3.3515102e-01,\n",
       "          9.5594394e-01,  3.8226959e-01, -7.5719631e-01, -9.3931991e-01,\n",
       "         -6.3558155e-01, -6.0732901e-01,  7.0057154e-01,  7.2361279e-01,\n",
       "          7.2901070e-01,  8.6588341e-01,  7.6453698e-01,  2.0882070e-01,\n",
       "         -6.9852591e-02, -5.6448980e-04,  9.9979931e-01, -4.4409996e-01,\n",
       "         -1.8067124e-01, -4.8985934e-01, -2.9143107e-01, -4.2540884e-01,\n",
       "         -1.9874960e-01,  1.0000000e+00,  3.5660189e-01,  7.7566123e-01,\n",
       "         -9.9382377e-01, -9.2807078e-01, -9.3173838e-01,  1.0000000e+00,\n",
       "          8.5004020e-01, -7.6071566e-01,  7.1803629e-01,  7.7546883e-01,\n",
       "         -1.7516181e-01,  8.0946916e-01, -3.3654737e-01, -3.0238512e-01,\n",
       "          4.5746776e-01,  3.0804363e-01,  9.7023195e-01, -6.1890370e-01,\n",
       "         -9.7572124e-01, -5.9494883e-01,  5.6339043e-01, -9.6665108e-01,\n",
       "          9.9998140e-01, -6.1034006e-01, -3.6057490e-01, -4.9643567e-01,\n",
       "         -4.9143556e-01,  4.4781744e-01,  2.8738467e-02, -9.8315477e-01,\n",
       "         -3.4738714e-01,  3.0911028e-01,  9.6663898e-01,  3.7586382e-01,\n",
       "         -6.4110661e-01, -8.9026487e-01,  8.9226872e-01,  8.3199972e-01,\n",
       "         -9.5913196e-01, -9.5776629e-01,  9.7116637e-01, -9.8497111e-01,\n",
       "          7.6781917e-01,  1.0000000e+00,  3.8399881e-01,  4.3805024e-01,\n",
       "          3.5229188e-01, -4.4613606e-01,  4.4656894e-01, -6.9063085e-01,\n",
       "          6.7442542e-01, -9.5915580e-01, -4.5328474e-01, -2.9615226e-01,\n",
       "          3.5768434e-01, -2.4115433e-01, -5.8831310e-01,  7.6330805e-01,\n",
       "          3.1366721e-01, -6.0310078e-01, -6.8479520e-01, -2.6014689e-01,\n",
       "          5.7515991e-01,  9.1684413e-01, -3.5680008e-01, -2.3155740e-01,\n",
       "          1.1572763e-01, -1.7711897e-01, -9.4756359e-01, -5.2314180e-01,\n",
       "         -6.0461760e-01, -9.9999863e-01,  5.4166734e-01, -1.0000000e+00,\n",
       "          6.6000140e-01,  3.3903599e-01, -2.5796208e-01,  8.9843398e-01,\n",
       "          3.5850316e-01,  7.8009164e-01, -8.6345613e-01, -9.0424347e-01,\n",
       "          2.3517458e-01,  8.4754229e-01, -4.8370484e-01, -7.7643692e-01,\n",
       "         -7.7708697e-01,  4.5154652e-01, -1.2064352e-01,  3.4533754e-01,\n",
       "         -7.5830430e-01,  7.3866343e-01, -2.5487801e-01,  1.0000000e+00,\n",
       "          1.5672636e-01, -6.4717263e-01, -9.8084658e-01,  3.2154456e-01,\n",
       "         -3.4947982e-01,  1.0000000e+00, -8.8808620e-01, -9.7075868e-01,\n",
       "          4.1761369e-01, -6.5950602e-01, -8.3906174e-01,  4.5644572e-01,\n",
       "          7.0838876e-02, -8.5964859e-01, -9.6872550e-01,  9.5658386e-01,\n",
       "          8.9531106e-01, -6.7916250e-01,  7.9199594e-01, -3.7720469e-01,\n",
       "         -5.9968197e-01,  1.8921909e-01,  9.3477023e-01,  9.8794460e-01,\n",
       "          7.0796472e-01,  9.2108780e-01, -1.5953988e-01, -4.8346728e-01,\n",
       "          9.7664040e-01,  2.9525197e-01,  5.3205329e-01,  3.2265782e-01,\n",
       "          1.0000000e+00,  4.9799159e-01, -9.3100083e-01, -3.2474363e-01,\n",
       "         -9.8284167e-01, -2.6799589e-01, -9.5216614e-01,  4.5391658e-01,\n",
       "          3.9437199e-01,  9.2619020e-01, -3.0975196e-01,  9.6936643e-01,\n",
       "         -9.4026351e-01,  1.6679834e-01, -8.3223271e-01, -7.0441061e-01,\n",
       "          5.4937023e-01, -9.3037343e-01, -9.8870248e-01, -9.9157214e-01,\n",
       "          7.3868239e-01, -5.2632725e-01, -9.2953205e-02,  2.7795574e-01,\n",
       "          2.5438562e-01,  5.5589265e-01,  5.7078004e-01, -1.0000000e+00,\n",
       "          9.5199955e-01,  5.8298081e-01,  9.1339386e-01,  9.7862440e-01,\n",
       "          7.4903190e-01,  7.3997158e-01,  3.7118635e-01, -9.8966074e-01,\n",
       "         -9.8484856e-01, -5.3139746e-01, -3.8897955e-01,  8.4941369e-01,\n",
       "          8.1701696e-01,  8.9269680e-01,  6.1689180e-01, -5.7528448e-01,\n",
       "         -2.8646687e-01, -7.6057017e-01, -7.7893907e-01, -9.9444175e-01,\n",
       "          5.7218784e-01, -7.7219456e-01, -9.5769626e-01,  9.6742070e-01,\n",
       "         -2.1797928e-01, -1.7555273e-01, -3.2655674e-01, -9.0677762e-01,\n",
       "          9.3559742e-01,  7.6623935e-01,  1.9059601e-01,  1.5392871e-01,\n",
       "          5.4021740e-01,  9.0248352e-01,  9.4038892e-01,  9.8888487e-01,\n",
       "         -9.1014379e-01,  7.8835994e-01, -8.3138257e-01,  6.1119503e-01,\n",
       "          8.2166892e-01, -9.4196767e-01,  3.7513283e-01,  5.4940528e-01,\n",
       "         -6.1531812e-01,  3.9150161e-01, -3.6662689e-01, -9.7475231e-01,\n",
       "          8.8887805e-01, -3.6283782e-01,  6.5320545e-01, -5.3506964e-01,\n",
       "         -2.2127867e-02, -4.4015801e-01, -3.8756707e-01, -7.8935486e-01,\n",
       "         -6.7090219e-01,  6.8736994e-01,  4.3467963e-01,  9.0751094e-01,\n",
       "          9.1395384e-01, -1.0754465e-01, -8.5499156e-01, -3.2296547e-01,\n",
       "         -7.8099227e-01, -9.3507546e-01,  9.5662099e-01, -2.4696708e-01,\n",
       "         -1.8467523e-01,  7.1814156e-01,  1.6683359e-01,  9.5427251e-01,\n",
       "          5.2027887e-01, -5.1134634e-01, -3.5843018e-01, -7.7673435e-01,\n",
       "          9.0137887e-01, -6.4581001e-01, -6.6820300e-01, -6.7957538e-01,\n",
       "          8.3088905e-01,  4.5693967e-01,  9.9999833e-01, -8.6158717e-01,\n",
       "         -9.5270836e-01, -5.7405442e-01, -4.7562289e-01,  5.1158577e-01,\n",
       "         -7.0260715e-01, -1.0000000e+00,  5.0562429e-01, -6.5204507e-01,\n",
       "          8.1373054e-01, -8.7213898e-01,  8.0931920e-01, -8.1822181e-01,\n",
       "         -9.8819637e-01, -4.0008223e-01,  3.3894491e-01,  7.6701325e-01,\n",
       "         -5.1635271e-01, -8.7594086e-01,  6.1595225e-01, -7.5270915e-01,\n",
       "          9.8932844e-01,  8.9571065e-01, -6.1424994e-01,  2.1965036e-01,\n",
       "          7.5275433e-01, -8.2038939e-01, -8.0569124e-01,  9.3803930e-01]],\n",
       "       dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 6, 768), dtype=float32, numpy=\n",
       " array([[[-0.0794746 ,  0.00580813, -0.31413996, ..., -0.45097223,\n",
       "           0.29333133,  0.2338771 ],\n",
       "         [ 0.3931604 ,  0.50336295,  0.24021342, ..., -0.32635614,\n",
       "           0.34986067,  0.20673294],\n",
       "         [ 0.35789207,  0.10767137, -0.04988797, ..., -0.5082267 ,\n",
       "           0.250488  , -0.26268718],\n",
       "         [-0.2989211 , -0.24708733,  0.07151413, ..., -0.33810022,\n",
       "           0.12699538, -0.09681825],\n",
       "         [-0.36815354, -0.71465224, -0.21032533, ...,  0.35395217,\n",
       "           0.33438578, -0.623348  ],\n",
       "         [ 0.8869218 , -0.16996951, -0.29173565, ...,  0.05816324,\n",
       "          -0.5775989 , -0.32075343]]], dtype=float32)>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sent = ['[CLS]'] + tokenizer.tokenize(\"Roses are red.\") + [\"[SEP]\"]\n",
    "my_input = [tf.expand_dims(tf.cast(get_ids(my_sent), tf.int32), 0), \n",
    "            tf.expand_dims(tf.cast(get_masks(my_sent), tf.int32), 0), \n",
    "            tf.expand_dims(tf.cast(get_segments(my_sent), tf.int32), 0)]\n",
    "\n",
    "bert_layer(my_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nonprofit-semester",
   "metadata": {},
   "source": [
    "Pass!\n",
    "\n",
    "bert_layer returns:\n",
    "1. Vector representation of the whole sentence, size=(768)\n",
    "2. Vector representation of each word, size=(6, 768)\n",
    "\n",
    "Note1: axis=0 indicates the batch, which is just 1 batch in our test case\n",
    "\n",
    "Note2: vector length is 768 because we're using BERT base model\n",
    "\n",
    "Next, shuffle the sentences and apply the 3 formatting functions to all sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "beneficial-fetish",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "equivalent-asian",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_sent_len = 7\n",
    "\n",
    "all_inputs = [([get_ids(sentence), get_masks(sentence), get_segments(sentence)], label)\n",
    "              for sentence, label in zip(data_sentences, data_labels)\n",
    "              if len(sentence) > short_sent_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-anchor",
   "metadata": {},
   "source": [
    "<a id=\"datagen\"></a>\n",
    "#### Data generator\n",
    "Generate batch of data:\n",
    "- Training data: size=(batch size, 3 (for the 3 inputs), padded sentence length)\n",
    "    - Note that each batch has different padded sentence length, but should not be too short!\n",
    "- Training label: size=(batch size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fitted-enemy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list is a type of iterator so it can be used as generator for a dataset\n",
    "data_gen = tf.data.Dataset.from_generator(lambda: all_inputs, output_types=(tf.int32, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "classical-ranch",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "batch_inputs = data_gen.padded_batch(BATCH_SIZE,\n",
    "                                     padded_shapes=((3, None), ()),\n",
    "                                     padding_values=(0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "smart-rhythm",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(32, 3, 8), dtype=int32, numpy=\n",
       " array([[[  101,  5742,  1029,  8501,  4402,  4402,  2098,   102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  2821, 23644,  4067,  2017,  8840,  2140,   102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  9875,  2026,  2047,  2880,  5440,  2773,   102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  4931,  1010,  4283,  2005,  3582,  5959,   102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  2736, 13869,  2005,  1012,  8038,  2100,   102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  2748,   999,  9789,  3238,  2003,  2006,   102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  1045,  2342,  2070, 13076, 13804,  4280,   102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  2183,  2000,  1996, 24385,  1012,  1012,   102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  101, 12476,   999,  2064,  1005,  1056,  3524,   102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  1045,  2031,  2042,  2039,  2005,  2847,   102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  2200,  3407,  2007,  2026,  5439,  2208,   102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  2154,  2819,  2008, 15478,  2001,  3835,   102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  2021,  1045,  2131,  6015,  2012,  2505,   102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  2589,  2115,  2057, 22499, 20058,  1012,   102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  2437,  9805, 18879,  4596,  2005,  3071,   102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  6203, 22140,  1010,  2339,  2025,  1029,   102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  3666, 13812,  1005,  1055,  2265,  3892,   102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  3666,  1037,  3185,   999,   999,   999,   102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  2006,  2115, 26322,  1029,  2033,  2205,   102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  1045,  2215,  1037,  9004, 28844,  2100,   102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  3374,  1045,  9554, 10308,  2019, 10777,   102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  2017,  2018,  2033,  2012,  8808, 12668,   102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  1045,  2215,  2000,  2022,  1999,  5759,   102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  2012,  1996,  3899,  2902,  2007,  4098,   102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  101, 10047,  2383,  3471,  2007,  2026,  3042,   102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  4954,  1998,  5458,  2012,  5747,  5822,   102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  1045,  3984,  4500,  2015,  2079,  9958,   102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  1045,  2288,  2178,  4658, 10474,  5896,   102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  1045,  1005,  1049,  2894,  1999,  2465,   102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  1045,  2074,  2481,  2102,  9507,  2009,   102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  1045,  2031,  1996,  5409, 14978,  2412,   102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0]],\n",
       " \n",
       "        [[  101,  1045,  2572,  1054,  1057,  2183,  1029,   102],\n",
       "         [    1,     1,     1,     1,     1,     1,     1,     1],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0]]])>,\n",
       " <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
       " array([0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 1, 0, 1, 0, 1])>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expect to see size (BATCH_SIZE, 3, sentence length)\n",
    "next(iter(batch_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "southern-egyptian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle batches and split training and testing set\n",
    "NUM_BATCHES = math.ceil(len(all_inputs) / BATCH_SIZE)\n",
    "test_size = 0.1\n",
    "NUM_BATCHES_TEST = int(NUM_BATCHES * test_size)\n",
    "\n",
    "batch_inputs.shuffle(NUM_BATCHES)\n",
    "test_inputs = batch_inputs.take(NUM_BATCHES_TEST)\n",
    "train_inputs = batch_inputs.skip(NUM_BATCHES_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-directive",
   "metadata": {},
   "source": [
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-bahrain",
   "metadata": {},
   "source": [
    "<a id=\"model\"></a>\n",
    "## 3. Modeling\n",
    "Use BERT for embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "moderate-brick",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCNNBERTEmbedding(tf.keras.Model):\n",
    "    def __init__(self, \n",
    "                 num_filters=50, \n",
    "                 FFN_units=512, \n",
    "                 num_classes=2, \n",
    "                 dropout_rate=0.1,\n",
    "                 name='dcnn'):\n",
    "        super(DCNNBERTEmbedding, self).__init__(name=name)\n",
    "        \n",
    "        # Define layers\n",
    "        \n",
    "        # Instead of tf.keras.layers.Embedding, use BERT layer (as is) for word embedding\n",
    "        self.bert_layer = hub.KerasLayer(bert_base_model, trainable=False)\n",
    "        \n",
    "        # Len 2 feature detector (bi gram)\n",
    "        self.bigram = layers.Conv1D(kernel_size=2, filters=num_filters, padding='valid', activation='relu')\n",
    "        \n",
    "        # Len 3 feature detector (tri gram)\n",
    "        self.trigram = layers.Conv1D(kernel_size=3, filters=num_filters, padding='valid', activation='relu')\n",
    "        \n",
    "        # Len 4 feature detector (four gram)\n",
    "        self.fourgram = layers.Conv1D(kernel_size=4, filters=num_filters, padding='valid', activation='relu')        \n",
    "        \n",
    "        # Pooling layer\n",
    "        self.pool = layers.GlobalMaxPooling1D()\n",
    "        \n",
    "        # Fully connected hidden layer\n",
    "        self.dense_1 = layers.Dense(units=FFN_units, activation='relu')\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        \n",
    "        # Fully connected final layer\n",
    "        if num_classes==2:\n",
    "            self.last_dense = layers.Dense(units=1, activation='sigmoid')\n",
    "        else:\n",
    "            self.last_dense = layers.Dense(units=num_classes, activation='softmax')\n",
    "            \n",
    "    def embd_with_bert(self, all_tokens):\n",
    "        # bert_layer returns:\n",
    "        # 1. BERT (vector) representation of whole sentence (not use here)\n",
    "        # 2. BERT (vector) representation of individual words \n",
    "        _, embs = self.bert_layer([all_tokens[:,0,:],  # ids\n",
    "                                   all_tokens[:,1,:],  # masks\n",
    "                                   all_tokens[:,2,:]]) # segments\n",
    "        return embs\n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        x = self.embd_with_bert(inputs)\n",
    "        x_1 = self.bigram(x)\n",
    "        x_1 = self.pool(x_1)\n",
    "        x_2 = self.trigram(x)\n",
    "        x_2 = self.pool(x_2)\n",
    "        x_3 = self.fourgram(x)\n",
    "        x_3 = self.pool(x_3) # shape = (batch_size, num_filters)\n",
    "        \n",
    "        merged = tf.concat([x_1, x_2, x_3], axis=-1) # shape = (batch_size, 3*num_filters)\n",
    "        merged = self.dense_1(merged)\n",
    "        merged = self.dropout(merged, training)\n",
    "        output = self.last_dense(merged)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-tunisia",
   "metadata": {},
   "source": [
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-sierra",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a>\n",
    "## 4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identified-trinidad",
   "metadata": {},
   "source": [
    "Compare to training model using tf.keras.layers.Embedding for word embedding, should expect:\n",
    "\n",
    "1. Faster training because we don't have to train embedding layer (use pretrained BERT embedding layer)\n",
    "2. Less overfitting\n",
    "    - Customized embedding layer is likely to overfits\n",
    "    - Pretrained BERT embedding layer (without fine tuning) is well trained and general enough for our dataset, hence less overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "neural-feeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "VOCAB_SIZE = len(tokenizer.vocab)\n",
    "EMB_DIM = 200\n",
    "NUM_FILTERS = 100\n",
    "FFN_UNITS = 256\n",
    "NUM_CLASSES = 2\n",
    "DROPOUT_RATE = 0.2\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "varied-configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "Dcnn = DCNNBERTEmbedding(num_filters=NUM_FILTERS, \n",
    "                         FFN_units=FFN_UNITS, \n",
    "                         num_classes=NUM_CLASSES, \n",
    "                         dropout_rate=DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "synthetic-threshold",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "if NUM_CLASSES == 2:\n",
    "    Dcnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "else:\n",
    "    Dcnn.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['sparse_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "incredible-connection",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file models\\embding already exists.\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p models\\embding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "developmental-arctic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no checkpoint available...\n"
     ]
    }
   ],
   "source": [
    "# Checkpoint manager\n",
    "checkpoint_path='models\\embding'\n",
    "ckpt = tf.train.Checkpoint(Dcnn=Dcnn)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)\n",
    "\n",
    "# If there is checkpoint in folder, restore latest checkpoint\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print(\"latest checkpoint restored!\")\n",
    "else:\n",
    "    print(\"no checkpoint available...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "unexpected-alarm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint manager callback at end of epoch\n",
    "class MyCustomCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"Save checkpoint at end of each epoch\"\"\"\n",
    "        ckpt_manager.save()\n",
    "        print(\"Checkpoint saved at {}\".format(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "injured-replication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', verbose=1, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "early-change",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "40625/40625 [==============================] - 36651s 901ms/step - loss: 0.3846 - accuracy: 0.8296 - val_loss: 0.3516 - val_accuracy: 0.8520\n",
      "Checkpoint saved at models\\embding\n",
      "Epoch 2/5\n",
      "40625/40625 [==============================] - 40538s 997ms/step - loss: 0.3495 - accuracy: 0.8490 - val_loss: 0.3426 - val_accuracy: 0.8553\n",
      "Checkpoint saved at models\\embding\n",
      "Epoch 3/5\n",
      "40625/40625 [==============================] - 36788s 905ms/step - loss: 0.3372 - accuracy: 0.8550 - val_loss: 0.3363 - val_accuracy: 0.8599\n",
      "Checkpoint saved at models\\embding\n",
      "Epoch 4/5\n",
      "40625/40625 [==============================] - 38958s 959ms/step - loss: 0.3283 - accuracy: 0.8592 - val_loss: 0.3308 - val_accuracy: 0.8604\n",
      "Checkpoint saved at models\\embding\n",
      "Epoch 5/5\n",
      "40625/40625 [==============================] - 37009s 911ms/step - loss: 0.3215 - accuracy: 0.8627 - val_loss: 0.3320 - val_accuracy: 0.8601\n",
      "Checkpoint saved at models\\embding\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00005: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x27d6c3d98b0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "Dcnn.fit(train_inputs,\n",
    "         epochs=EPOCHS,\n",
    "         validation_data=test_inputs,\n",
    "         callbacks=[MyCustomCallback(), early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-style",
   "metadata": {},
   "source": [
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "communist-attempt",
   "metadata": {},
   "source": [
    "<a id=\"eval\"></a>\n",
    "## 5. Evaluation\n",
    "Note that we're using pretrained bert AS IS!! So it's already good for not fine tuning with our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "tested-figure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4513/4513 [==============================] - 1550s 343ms/step - loss: 0.3308 - accuracy: 0.8604\n",
      "[0.3308480978012085, 0.8604309558868408]\n"
     ]
    }
   ],
   "source": [
    "predictions = Dcnn.evaluate(test_inputs)\n",
    "print(predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fixed-westminster",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(sentence):\n",
    "    tokens = encode_sentence(sentence)\n",
    "    \n",
    "    inputs = [tf.cast(get_ids(tokens), dtype=tf.int32),\n",
    "              tf.cast(get_mask(tokens), dtype=tf.int32),\n",
    "              tf.cast(get_segments(tokens), dtype=tf.int32)]\n",
    "\n",
    "    inputs = tf.stack(inputs, axis=0)\n",
    "    inputs = tf.expand_dims(inputs, axis=0) # simulates a batch\n",
    "\n",
    "    output = Dcnn(inputs, training=False)\n",
    "    sentiment = math.floor(output*2)\n",
    "    \n",
    "    print(f\"Output of the model: {output}\\nPredicted sentiment:\", \"Positive\" if sentiment else \"Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "above-raising",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of the model: [[0.20388371]]\n",
      "Predicted sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "get_prediction(\"Never am I gonna train this again!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "durable-luxury",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of the model: [[0.9463612]]\n",
      "Predicted sentiment: Positive\n"
     ]
    }
   ],
   "source": [
    "get_prediction(\"Let's do another BERT LOL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-simple",
   "metadata": {},
   "source": [
    "[Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-bradford",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-prerequisite",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
